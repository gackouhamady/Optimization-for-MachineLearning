# Projet : Optimisation pour l'Apprentissage Machine (MLSD-AMSD 2024/2025)

## ğŸ“š PrÃ©sentation

Ce projet, rÃ©alisÃ© dans le cadre du Master 1 MLSD-AMSD (UniversitÃ© Paris CitÃ©) encadrÃ© par **Lazhar Labiod**, explore deux grandes thÃ©matiques clÃ©s en optimisation appliquÃ©e Ã  l'apprentissage automatique :
- **DÃ©composition Matricielle (SVD)** pour rÃ©organisation et visualisation de donnÃ©es.
- **Comparaison de mÃ©thodes d'optimisation** basÃ©es sur la descente de gradient et Newton sur diffÃ©rents jeux de donnÃ©es.

## ğŸ”„ Objectifs

- ImplÃ©menter et comparer la **SVD classique** et la **mÃ©thode de la puissance itÃ©rÃ©e**.
- Visualiser des matrices de donnÃ©es rÃ©ordonnÃ©es pour mieux interprÃ©ter leur structure interne.
- ImplÃ©menter et Ã©valuer plusieurs **algorithmes d'optimisation** (BGD, SGD, MiniBatch, Newton, Momentum, Nesterov, Adagrad, RMSProp, Adam, AdamW).
- Tester la sensibilitÃ© aux hyperparamÃ¨tres.

## ğŸ”¹ Structure du projet

### Partie 1 : DÃ©composition Matricielle et Visualisation
- **ImplÃ©mentation de la SVD par la mÃ©thode des puissances**
- **Analyse de reconstruction de matrices**
- **VÃ©rification du thÃ©orÃ¨me d'Eckart-Young**
- **Comparaison entre vecteurs propres et vecteurs singuliers**
- **Application de R1SVD pour rÃ©organisation de matrices**
- **Visualisation graphique des matrices rÃ©ordonnÃ©es**

### Partie 2 : Optimisation pour la RÃ©gression Logistique
- **ImplÃ©mentation from scratch** de :
  - Batch Gradient Descent (BGD)
  - Stochastic Gradient Descent (SGD)
  - Mini-Batch Gradient Descent
  - Descente de gradient avec Momentum
  - Nesterov Accelerated Gradient
  - Newton Method
  - Adagrad
  - RMSProp
  - Adam
  - AdamW

- **Comparaison expÃ©rimentale sur 4 jeux de donnÃ©es** :
  - Haberman (peu d'exemples, peu de features)
  - Sonar (peu d'exemples, beaucoup de features)
  - Covertype (beaucoup d'exemples, beaucoup de features)
  - Mushroom (beaucoup d'exemples, peu de features)

- **SensibilitÃ© aux hyperparamÃ¨tres**
  - Learning Rate
  - Batch Size
  - Momentum / Beta

## ğŸ” MÃ©thodologie DÃ©taillÃ©e

- **SVD (Singular Value Decomposition)** :
  - Calcul par puissance itÃ©rÃ©e
  - Reconstruction de matrices et mesure de l'erreur (Frobenius norm)
  - Test du thÃ©orÃ¨me d'Eckart-Young
  - Application Ã  la rÃ©organisation de matrices de similaritÃ©

- **Optimisation de la RÃ©gression Logistique** :
  - Fonction CoÃ»t : Log Loss
  - Descente avec diverses stratÃ©gies d'optimisation
  - Ã‰valuation par convergence du coÃ»t, temps d'exÃ©cution, stabilitÃ©

- **Analyse ExpÃ©rimentale** :
  - Visualisation graphique (matplotlib, seaborn)
  - Tableaux rÃ©capitulatifs (tabulate)

## ğŸ› ï¸ Installation

PrÃ©requis Python >= 3.8 :

```bash
pip install numpy pandas matplotlib seaborn scikit-learn tqdm tabulate
```

## ğŸ”¢ ExÃ©cution

Ouvrir le projet dans Jupyter Notebook et exÃ©cuter les cellules dans l'ordre des sections.

```bash
jupyter notebook Projet_Opti_ML_24_FINAL.ipynb
```

## ğŸ“ Organisation des fichiers

```text
Projet_Opti_ML_24_FINAL.ipynb
â”œâ”€â”€ Partie 1 : SVD et Visualisation
â”‚   â”œâ”€â”€ DÃ©composition Matricielle
â”‚   â”œâ”€â”€ Reconstruction et Analyse d'erreurs
â”‚   â””â€” RÃ©organisation des matrices
â”œâ”€â”€ Partie 2 : Optimisation de ModÃ¨les
    â”œâ”€â”€ ImplÃ©mentation de 10 variantes d'optimisation
    â””â€” Comparaison sur 4 datasets + Analyse de sensibilitÃ©
```

## ğŸ‘¨â€ğŸ’¼ Auteurs

- Bastien HOTTELET
- Hamady GACKOU
- Omar NAMOUS

Projet supervisÃ© par **Lazhar Labiod**.

